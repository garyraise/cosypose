{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77b8139",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Reproduce the error for image loading\n",
    "- Assert cam_R_w2c with reprojection\n",
    "- How can I use @pytest.fixture and unit_test.mock (mock = MagicMock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d9f8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from collections import OrderedDict\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import logging\n",
    "from cosypose.config import EXP_DIR, MEMORY, RESULTS_DIR, LOCAL_DATA_DIR\n",
    "\n",
    "import cosypose.utils.tensor_collection as tc\n",
    "\n",
    "from cosypose.datasets.datasets_cfg import make_scene_dataset, make_object_dataset\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72950099",
   "metadata": {},
   "source": [
    "from cosypose.datasets.bop import remap_bop_targets\n",
    "from cosypose.datasets.wrappers.multiview_wrapper import MultiViewWrapper\n",
    "\n",
    "from cosypose.datasets.samplers import ListSampler\n",
    "from cosypose.lib3d.rigid_mesh_database import MeshDataBase\n",
    "from cosypose.training.pose_models_cfg import create_model_refiner, create_model_coarse, check_update_config\n",
    "from cosypose.rendering.bullet_batch_renderer import BulletBatchRenderer\n",
    "from cosypose.integrated.pose_predictor import CoarseRefinePosePredictor\n",
    "from cosypose.integrated.multiview_predictor import MultiviewScenePredictor\n",
    "\n",
    "from cosypose.evaluation.meters.pose_meters import PoseErrorMeter\n",
    "from cosypose.evaluation.pred_runner.multiview_predictions import MultiviewPredictionRunner\n",
    "from cosypose.evaluation.eval_runner.pose_eval import PoseEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "230b5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 8\n",
    "n_plotters = 8\n",
    "n_views = 1\n",
    "\n",
    "n_frames = None\n",
    "scene_id = None\n",
    "group_id = None\n",
    "n_groups = None\n",
    "skip_mv = n_views < 2\n",
    "skip_predictions = False\n",
    "\n",
    "object_set = 'bracket_assembly'\n",
    "coarse_run_id = f'bracket_assembly_coarse-transnoise-zxyavg-616093'\n",
    "refiner_run_id = 'bracket_assembly_refiner--558735'\n",
    "n_coarse_iterations = 1\n",
    "n_refiner_iterations = 2\n",
    "config = ds_name = 'bracket_assembly'\n",
    "comment = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9c56498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:57:48.369978 - Building index and loading annotations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]169068.5s, 2817.8min: Loading build_index...\n"
     ]
    }
   ],
   "source": [
    "scene_id = 0\n",
    "n_rand = np.random.randint(1e10)\n",
    "save_dir = RESULTS_DIR / f'{config}-n_views={n_views}-{comment}-{n_rand}'\n",
    "scene_ds = make_scene_dataset(ds_name)\n",
    "mask = scene_ds.frame_index['scene_id'] == scene_id\n",
    "scene_ds.frame_index = scene_ds.frame_index[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4822a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosypose.utils.logging import get_logger\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35b625d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_detection_from_gt():\n",
    "    \n",
    "    path_data_dir = LOCAL_DATA_DIR / 'bop_datasets' / 'bracket_assembly'\n",
    "    path_scene_dir = os.path.join(path_data_dir, \"train_pbr\")\n",
    "    scene_names = os.listdir(path_scene_dir)\n",
    "    infos, poses, bboxes = [], [], []\n",
    "    # debug_only = 0\n",
    "    for scene_id, scene_name in enumerate(scene_names):\n",
    "        path_scene_gt_info = os.path.join(path_scene_dir, scene_name, \"scene_gt_info.json\")\n",
    "        path_scene_gt = os.path.join(path_scene_dir, scene_name, \"scene_gt.json\")\n",
    "        path_scene_gt_camera = os.path.join(path_scene_dir, scene_name, \"scene_camera.json\")\n",
    "        with open(path_scene_gt_info, \"r\") as f:\n",
    "            json_data_gt_info = json.load(f)\n",
    "        with open(path_scene_gt, \"r\") as f:\n",
    "            json_data_gt = json.load(f)\n",
    "        with open(path_scene_gt_camera, \"r\") as f:\n",
    "            json_gt_camera = json.load(f)\n",
    "        img_names_rgb = os.listdir(os.path.join(path_scene_dir, scene_name, \"rgb\"))\n",
    "        # todo\n",
    "        # scene_camera = load_scene_camera(path_scene_gt_camera)\n",
    "        from cosypose.lib3d import Transform\n",
    "        for img_id, img_name in enumerate(img_names_rgb[:-1]):\n",
    "            if not f\"{img_id}\" in json_data_gt_info:\n",
    "                continue\n",
    "            if not f\"{img_id}\" in json_data_gt:\n",
    "                continue\n",
    "            if not f\"{img_id}\" in json_gt_camera:\n",
    "                continue\n",
    "            # cam_R_w2c = scene_camera[img_id][\"cam_R_w2c\"]\n",
    "            # TODO\n",
    "            cam_R_w2c = json_gt_camera[f\"{img_id}\"][\"cam_R_w2c\"]\n",
    "            cam_t_w2c = json_gt_camera[f\"{img_id}\"][\"cam_t_w2c\"]\n",
    "            row0 = [cam_R_w2c[0], cam_R_w2c[1], cam_R_w2c[2], cam_t_w2c[0]] \n",
    "            row1 = [cam_R_w2c[3], cam_R_w2c[4], cam_R_w2c[5], cam_t_w2c[1]]\n",
    "            row2 = [cam_R_w2c[6], cam_R_w2c[7], cam_R_w2c[8], cam_t_w2c[2]]           \n",
    "            row3 = [0, 0, 0, 1]\n",
    "            cam_rot_loc_mat = np.asarray([row0, row1, row2, row3])\n",
    "\n",
    "\n",
    "            if 'cam_R_w2c' in json_gt_camera[f\"{img_id}\"]:\n",
    "                RC0 = np.array(json_gt_camera[f\"{img_id}\"]['cam_R_w2c']).reshape(3, 3)\n",
    "                tC0 = np.array(json_gt_camera[f\"{img_id}\"]['cam_t_w2c'])# * 0.001\n",
    "                TC0 = Transform(RC0, tC0)\n",
    "                T0C = TC0.inverse()\n",
    "                T0C = T0C.toHomogeneousMatrix()\n",
    "            for label_idx, label in enumerate(json_data_gt[f\"{img_id}\"]):\n",
    "                obj_id = label[\"obj_id\"] # int\n",
    "                list_bbox = json_data_gt_info[f\"{img_id}\"][label_idx][\"bbox_obj\"] # TODO: ?\n",
    "                xmin = list_bbox[0]\n",
    "                ymin = list_bbox[1]\n",
    "                xmax = list_bbox[2]\n",
    "                ymax = list_bbox[1] + list_bbox[3]\n",
    "                list_bbox = [xmin, ymin, xmax, ymax]\n",
    "                list_rot  = json_data_gt[f\"{img_id}\"][label_idx][\"cam_R_m2c\"]\n",
    "                list_loc  = json_data_gt[f\"{img_id}\"][label_idx][\"cam_t_m2c\"]\n",
    "                # RCO = np.array(json_data_gt[f\"{img_id}\"][label_idx]['cam_R_m2c']).reshape(3, 3)\n",
    "                # tCO = np.array(json_data_gt[f\"{img_id}\"][label_idx]['cam_t_m2c']) #* 0.001\n",
    "                # TCO = Transform(RCO, tCO)\n",
    "                # T0O = T0C * TCO\n",
    "                # T0O = T0O.toHomogeneousMatrix()\n",
    "\n",
    "                row0 = [list_rot[0], list_rot[1], list_rot[2], list_loc[0]] \n",
    "                row1 = [list_rot[3], list_rot[4], list_rot[5], list_loc[1]]\n",
    "                row2 = [list_rot[6], list_rot[7], list_rot[8], list_loc[2]]\n",
    "                row3 = [0, 0, 0, 1]\n",
    "                rot_loc_mat = np.asarray([row0, row1, row2, row3])\n",
    "                rot_loc_mat = np.matmul(np.linalg.inv(cam_rot_loc_mat), rot_loc_mat)\n",
    "                # if scene_id == 0 and img_id == 20:\n",
    "                #     print(\"label_idx\",label_idx)\n",
    "                #     print(\"rot_loc_mat\", rot_loc_mat)\n",
    "                infos.append(dict(\n",
    "                        scene_id=scene_id,\n",
    "                        view_id=img_id,\n",
    "                        score=1,\n",
    "                        label=f\"obj_{obj_id:06d}\",\n",
    "                    ))\n",
    "                # poses.append(T0O)\n",
    "                poses.append(rot_loc_mat)\n",
    "                bboxes.append(list_bbox)\n",
    "    data = tc.PandasTensorCollection(\n",
    "        infos=pd.DataFrame(infos),\n",
    "        poses=torch.as_tensor(np.stack(poses)).float(),\n",
    "        bboxes=torch.as_tensor(np.stack(bboxes)).float(),\n",
    "    ).cpu()\n",
    "    return data\n",
    "bracket_detections = load_custom_detection_from_gt(ds_name).cpu()\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8527540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "LOCAL_DATA = Path('/home/ubuntu/synthetic_pose_estimation/cosypose/local_data')\n",
    "DEBUG_DATA_DIR = LOCAL_DATA / 'debug_data'\n",
    "debug_file = open(DEBUG_DATA_DIR/'debug_iter=1.pth.tar','rb')\n",
    "debug_data = torch.load(debug_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f23f307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.1176, 0.1333, 0.1373,  ..., 0.0824, 0.0706, 0.0824],\n",
       "          [0.1412, 0.1451, 0.1451,  ..., 0.0824, 0.0980, 0.1216],\n",
       "          [0.1529, 0.1490, 0.1412,  ..., 0.0863, 0.0941, 0.1176],\n",
       "          ...,\n",
       "          [0.1020, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0980, 0.1255, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "          [0.0824, 0.1137, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4039, 0.4196, 0.4314,  ..., 0.5647, 0.5961, 0.5529],\n",
       "          [0.4353, 0.4392, 0.4392,  ..., 0.5647, 0.6118, 0.5922],\n",
       "          [0.4471, 0.4431, 0.4353,  ..., 0.5608, 0.6078, 0.5882],\n",
       "          ...,\n",
       "          [0.4157, 0.3529, 0.0941,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3686, 0.3098, 0.0667,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          [0.3176, 0.2627, 0.0314,  ..., 0.0000, 0.0000, 0.0039]],\n",
       "\n",
       "         [[0.4314, 0.4471, 0.4549,  ..., 0.6863, 0.7098, 0.6510],\n",
       "          [0.4588, 0.4627, 0.4627,  ..., 0.6863, 0.7294, 0.6902],\n",
       "          [0.4706, 0.4667, 0.4588,  ..., 0.6824, 0.7255, 0.6863],\n",
       "          ...,\n",
       "          [0.5529, 0.4471, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4824, 0.3804, 0.0824,  ..., 0.0078, 0.0000, 0.0000],\n",
       "          [0.4510, 0.3529, 0.0549,  ..., 0.0078, 0.0000, 0.0000]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f042d60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14917"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bracket_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a14e8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(coarse_run_id, refiner_run_id=None, n_workers=8, object_set='tless'):\n",
    "    if object_set == 'bracket_assembly':\n",
    "        object_ds_name, urdf_ds_name = 'bracket_assembly', 'bracket_assembly'\n",
    "    \n",
    "    object_ds = make_object_dataset(object_ds_name)\n",
    "    mesh_db = MeshDataBase.from_object_ds(object_ds)\n",
    "    renderer = BulletBatchRenderer(object_set=urdf_ds_name, n_workers=n_workers)\n",
    "    mesh_db_batched = mesh_db.batched().cuda()\n",
    "\n",
    "    def load_model(run_id):\n",
    "        if run_id is None:\n",
    "            return\n",
    "        run_dir = EXP_DIR / run_id\n",
    "        cfg = yaml.unsafe_load((run_dir / 'config.yaml').read_text())\n",
    "        cfg = check_update_config(cfg)\n",
    "        if cfg.train_refiner:\n",
    "            model = create_model_refiner(cfg, renderer=renderer, mesh_db=mesh_db_batched)\n",
    "            ckpt = torch.load(run_dir / 'checkpoint.pth.tar')\n",
    "        else:\n",
    "            model = create_model_coarse(cfg, renderer=renderer, mesh_db=mesh_db_batched)\n",
    "            ckpt = torch.load(run_dir / 'checkpoint.pth.tar')\n",
    "        ckpt = ckpt['state_dict']\n",
    "        model.load_state_dict(ckpt)\n",
    "        model = model.cuda().eval()\n",
    "        model.cfg = cfg\n",
    "        return model\n",
    "\n",
    "    coarse_model = load_model(coarse_run_id)\n",
    "    refiner_model = load_model(refiner_run_id)\n",
    "    model = CoarseRefinePosePredictor(coarse_model=coarse_model,\n",
    "                                      refiner_model=refiner_model,\n",
    "                                      bsz_objects=1)\n",
    "    return model, mesh_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bc6756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 20 # None\n",
    "if scene_id is not None:\n",
    "    mask = scene_ds.frame_index['scene_id'] == scene_id\n",
    "    scene_ds.frame_index = scene_ds.frame_index[mask].reset_index(drop=True)\n",
    "if n_frames is not None:\n",
    "    # scene_ds.frame_index = scene_ds.frame_index.reset_index(drop=True)[:n_frames]\n",
    "    scene_ds.frame_index = scene_ds.frame_index.reset_index(drop=True)[n_frames:n_frames+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4918623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scene_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b51c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_meters(scene_ds):\n",
    "    ds_name = scene_ds.name\n",
    "\n",
    "    compute_add = False\n",
    "    spheres_overlap_check = True\n",
    "    large_match_threshold_diameter_ratio = 0.5\n",
    "    print(\"ds_name\", ds_name)\n",
    "    if ds_name == 'tless.primesense.test.bop19':\n",
    "        targets_filename = 'test_targets_bop19.json'\n",
    "        visib_gt_min = -1\n",
    "        n_top = -1  # Given by targets\n",
    "    elif ds_name == 'tless.primesense.test':\n",
    "        targets_filename = 'all_target_tless.json'\n",
    "        n_top = 1\n",
    "        visib_gt_min = 0.1\n",
    "    elif 'ycbv' in ds_name:\n",
    "        compute_add = True\n",
    "        visib_gt_min = -1\n",
    "        targets_filename = None\n",
    "        n_top = 1\n",
    "        spheres_overlap_check = False\n",
    "    elif 'bracket_assembly' in ds_name:\n",
    "        targets_filename = None\n",
    "        visib_gt_min = -1\n",
    "        n_top = 1  # Given by targets\n",
    "        spheres_overlap_check = False\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    if 'tless' in ds_name:\n",
    "        object_ds_name = 'tless.eval'\n",
    "    elif 'ycbv' in ds_name:\n",
    "        object_ds_name = 'ycbv.bop-compat.eval'  # This is important for definition of symmetric objects\n",
    "    elif 'bracket_assembly' in ds_name:\n",
    "        object_ds_name = 'bracket_assembly'\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    if targets_filename is not None:\n",
    "        targets_path = scene_ds.ds_dir / targets_filename\n",
    "        targets = pd.read_json(targets_path)\n",
    "        targets = remap_bop_targets(targets)\n",
    "    else:\n",
    "        targets = None\n",
    "\n",
    "    object_ds = make_object_dataset(object_ds_name)\n",
    "    print(\"object_ds_name\", object_ds_name)\n",
    "    mesh_db = MeshDataBase.from_object_ds(object_ds)\n",
    "\n",
    "    error_types = ['ADD-S'] + (['ADD(-S)'] if compute_add else [])\n",
    "\n",
    "    base_kwargs = dict(\n",
    "        mesh_db=mesh_db,\n",
    "        # exact_meshes=True,\n",
    "        # sample_n_points=None,\n",
    "        exact_meshes=False,\n",
    "        sample_n_points=100,\n",
    "        errors_bsz=1,\n",
    "\n",
    "        # BOP-Like parameters\n",
    "        n_top=n_top,\n",
    "        visib_gt_min=visib_gt_min,\n",
    "        targets=targets,\n",
    "        spheres_overlap_check=spheres_overlap_check,\n",
    "    )\n",
    "\n",
    "    meters = dict()\n",
    "    for error_type in error_types:\n",
    "        # For measuring ADD-S AUC on T-LESS and average errors on ycbv/tless.\n",
    "        meters[f'{error_type}_ntop=BOP_matching=OVERLAP'] = PoseErrorMeter(\n",
    "            error_type=error_type, consider_all_predictions=False,\n",
    "            match_threshold=large_match_threshold_diameter_ratio,\n",
    "            report_error_stats=True, report_error_AUC=True, **base_kwargs)\n",
    "\n",
    "        if 'ycbv' in ds_name:\n",
    "            # For fair comparison with PoseCNN/DeepIM on YCB-Video ADD(-S) AUC\n",
    "            meters[f'{error_type}_ntop=1_matching=CLASS'] = PoseErrorMeter(\n",
    "                error_type=error_type, consider_all_predictions=False,\n",
    "                match_threshold=np.inf,\n",
    "                report_error_stats=False, report_error_AUC=True, **base_kwargs)\n",
    "\n",
    "        if 'tless' in ds_name:\n",
    "            meters.update({f'{error_type}_ntop=BOP_matching=BOP':  # For ADD-S<0.1d\n",
    "                           PoseErrorMeter(error_type=error_type, match_threshold=0.1, **base_kwargs),\n",
    "\n",
    "                           f'{error_type}_ntop=ALL_matching=BOP':  # For mAP\n",
    "                           PoseErrorMeter(error_type=error_type, match_threshold=0.1,\n",
    "                                          consider_all_predictions=True,\n",
    "                                          report_AP=True, **base_kwargs)})\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0737ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:57:55.099552 - Backbone: efficientnet-b3\n",
      "1 day, 22:57:55.534823 - Backbone: efficientnet-b3\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EGL 1.5 after reload.\n",
      "Loaded EGL 1.5 after reload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EGL 1.5 after reload.\n",
      "Loaded EGL 1.5 after reload.\n",
      "Loaded EGL 1.5 after reload.\n",
      "Loaded EGL 1.5 after reload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n",
      "EGL device choice: 0 of 4 (from EGL_VISIBLE_DEVICES)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EGL 1.5 after reload.\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "Destroy EGL OpenGL window.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "multiview wrapper get item 0 [0] scene_id           0\n",
      "view_ids        [20]\n",
      "n_views            1\n",
      "scene_ds_ids     [0]\n",
      "group_id           0\n",
      "Name: 0, dtype: object\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:57:57.590342 - batched_model_predictions torch.Size([1, 3, 540, 720]) 1 \n",
      "1 day, 22:57:57.593101 - batched_model_predictions tensor([0]) [0] torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiview predict torch.Size([1, 3, 540, 720])\n",
      "get_predictions torch.Size([1, 3, 540, 720]) None 2\n",
      "pose model torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:01.263319 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.295863 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000,  0.0083],\n",
      "         [ 0.0000,  0.0000, -1.0000, -0.0168],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.1202],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.2365,  0.5946,  0.7685, -0.2927],\n",
      "         [-0.9494, -0.3098, -0.0524, -0.3871],\n",
      "         [ 0.2069, -0.7419,  0.6378,  0.3032],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[ 53.9720,   0.0000, 155.7807],\n",
      "         [  0.0000,  53.9720, 127.0675],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[  1.1109,  -0.5788,  -1.3861,  -2.1547,   0.0769,  -2.1093, -55.8145,\n",
      "         -61.3351,   2.5234]], device='cuda:0')}, 'boxes_rend': tensor([[  -85.7894, -2263.6277,   642.6074,   186.6211]], device='cuda:0'), 'boxes_crop': tensor([[-3884.4053, -3194.6787,  4805.4053,  3322.6787]], device='cuda:0')}}\n",
      "1 day, 22:58:01.298167 - batched_model_predictions tensor([1]) [0] torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:01.410883 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.430305 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000, -0.0160],\n",
      "         [ 0.0000,  0.0000, -1.0000,  0.0058],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.1988],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.4660,  0.5019,  0.7287, -2.2609],\n",
      "         [-0.7560, -0.6538, -0.0331, -1.9796],\n",
      "         [ 0.4598, -0.5663,  0.6840,  1.7072],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[369.0375,   0.0000, 189.2115],\n",
      "         [  0.0000, 369.0374, 108.7988],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[   3.6996,   -4.8191,   -4.1746,  -17.8867,   -1.2663,  -20.2102,\n",
      "         -459.0229, -438.6347,    8.5885]], device='cuda:0')}, 'boxes_rend': tensor([[182.0855, -28.4171, 300.5348, 313.7751]], device='cuda:0'), 'boxes_crop': tensor([[-393.9451, -164.5839,  876.9451,  788.5839]], device='cuda:0')}}\n",
      "1 day, 22:58:01.432566 - batched_model_predictions tensor([2]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:01.473716 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.488533 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  3.7464e-04],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+00, -1.5562e-03],\n",
      "         [-1.0000e+00,  0.0000e+00,  0.0000e+00, -2.8158e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'TCO_output': tensor([[[-0.1799,  0.7491,  0.6376,  0.0030],\n",
      "         [-0.9803, -0.1904, -0.0529, -0.0016],\n",
      "         [ 0.0818, -0.6345,  0.7686, -0.0745],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[830.5667,   0.0000, 417.4795],\n",
      "         [  0.0000, 830.5667, 328.1529],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[  1.1006,  -0.2798,  -0.9323,  -0.6931,   0.0386,  -1.0625, -22.1495,\n",
      "         -28.6100,   2.6445]], device='cuda:0')}, 'boxes_rend': tensor([[-297.4109, -236.4964,  105.9244,   39.4536]], device='cuda:0'), 'boxes_crop': tensor([[-378.0798, -310.4511,  186.6023,  113.0604]], device='cuda:0')}}\n",
      "1 day, 22:58:01.490703 - batched_model_predictions tensor([3]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:01.528493 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.546421 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000, -0.1890],\n",
      "         [ 0.0000,  0.0000, -1.0000,  0.2915],\n",
      "         [-1.0000,  0.0000,  0.0000,  3.9012],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.2203,  0.5917,  0.7755, -0.3888],\n",
      "         [-0.9624, -0.2613, -0.0741,  0.5665],\n",
      "         [ 0.1588, -0.7627,  0.6270,  7.7759],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[ 2.1074e+04,  0.0000e+00,  1.1804e+03],\n",
      "         [ 0.0000e+00,  2.1074e+04, -1.4549e+03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[  0.8794,  -0.3883,  -1.1335,  -1.2280,   0.0808,  -1.2143, -32.7130,\n",
      "         -39.0479,   1.9932]], device='cuda:0')}, 'boxes_rend': tensor([[283.9573, 374.9834, 296.4485, 379.4381]], device='cuda:0'), 'boxes_crop': tensor([[277.3722, 370.6541, 299.6278, 387.3459]], device='cuda:0')}}\n",
      "1 day, 22:58:01.548770 - batched_model_predictions tensor([4]) [0] torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:01.666814 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.682831 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000, -0.0133],\n",
      "         [ 0.0000,  0.0000, -1.0000, -0.0040],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.0670],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.1651,  0.6757,  0.7185, -0.0635],\n",
      "         [-0.9824, -0.1770, -0.0593, -0.0513],\n",
      "         [ 0.0871, -0.7156,  0.6930,  0.1478],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[122.4163,   0.0000, 185.6634],\n",
      "         [  0.0000, 122.4163, 131.8471],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[  0.9872,  -0.2586,  -1.0456,  -0.9577,   0.0671,  -1.0577, -28.3468,\n",
      "         -35.1787,   2.2047]], device='cuda:0')}, 'boxes_rend': tensor([[-465.2318,  725.4101,  558.1038, 1147.8969]], device='cuda:0'), 'boxes_crop': tensor([[-1869.3634, -1315.0419,  1961.8746,  1558.3866]], device='cuda:0')}}\n",
      "1 day, 22:58:01.685034 - batched_model_predictions tensor([5]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:01.723115 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.740478 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000, -0.0290],\n",
      "         [ 0.0000,  0.0000, -1.0000, -0.0262],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.1517],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.3852,  0.5989,  0.7021, -0.7699],\n",
      "         [-0.8390, -0.5441,  0.0039, -0.7417],\n",
      "         [ 0.3843, -0.5875,  0.7121,  0.7183],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[257.6459,   0.0000, 208.7213],\n",
      "         [  0.0000, 257.6459, 163.9749],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[   2.4973,   -2.2688,   -2.4498,   -8.8338,   -0.4081,   -9.7448,\n",
      "         -226.9663, -221.5946,    4.7347]], device='cuda:0')}, 'boxes_rend': tensor([[-175.5810,  -15.8410,  174.7323,  504.0942]], device='cuda:0'), 'boxes_crop': tensor([[-830.6758, -666.1319,  989.6758,  699.1319]], device='cuda:0')}}\n",
      "1 day, 22:58:01.742785 - batched_model_predictions tensor([6]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:01.850303 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:01.866881 - outputs, {'iteration=1': {'TCO_input': tensor([[[ 0.0000,  1.0000,  0.0000, -0.0219],\n",
      "         [ 0.0000,  0.0000, -1.0000,  0.0191],\n",
      "         [-1.0000,  0.0000,  0.0000,  0.1757],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.3751,  0.5573,  0.7408, -0.8288],\n",
      "         [-0.8460, -0.5325, -0.0277, -0.6108],\n",
      "         [ 0.3790, -0.6371,  0.6712,  0.8562],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[266.6345,   0.0000, 192.7010],\n",
      "         [  0.0000, 266.6345,  90.4833],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[   2.2467,   -2.1469,   -2.5684,   -8.7128,   -0.4549,   -9.6379,\n",
      "         -224.8946, -219.2330,    4.8738]], device='cuda:0')}, 'boxes_rend': tensor([[-26.2454, 437.9729, 261.3578, 900.1567]], device='cuda:0'), 'boxes_crop': tensor([[-702.4926, -230.6194, 1056.4926, 1088.6194]], device='cuda:0')}}\n",
      "1 day, 22:58:01.870229 - batched_model_predictions torch.Size([1, 3, 540, 720]) 1 \n",
      "1 day, 22:58:01.872279 - batched_model_predictions tensor([0]) [0] torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:01.915756 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:02.045552 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:02.072186 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.2365,  0.5946,  0.7685, -0.2927],\n",
      "         [-0.9494, -0.3098, -0.0524, -0.3871],\n",
      "         [ 0.2069, -0.7419,  0.6378,  0.3032],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.2253,  0.5593,  0.7978, -0.6481],\n",
      "         [-0.9088, -0.4158,  0.0349, -0.7361],\n",
      "         [ 0.3512, -0.7172,  0.6019,  2.2181],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[142.2791,   0.0000, 296.8316],\n",
      "         [  0.0000, 142.2791, 301.1387],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 30.7047,  -0.1702,  -1.4422, -10.8007,  14.4379,  -1.5678,  95.7568,\n",
      "         134.4199,   7.3144]], device='cuda:0')}, 'boxes_rend': tensor([[-1108.5493, -2435.4429,  -272.9308,  -718.6356]], device='cuda:0'), 'boxes_crop': tensor([[-2703.3691, -2837.7375,   593.0118,  -365.4519]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-0.2253,  0.5593,  0.7978, -0.6481],\n",
      "         [-0.9088, -0.4158,  0.0349, -0.7361],\n",
      "         [ 0.3512, -0.7172,  0.6019,  2.2181],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.2049,  0.5199,  0.8293, -3.2076],\n",
      "         [-0.8335, -0.5369,  0.1306, -3.3069],\n",
      "         [ 0.5132, -0.6644,  0.5433, 15.2106],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[1.1300e+03, 0.0000e+00, 4.8969e+02],\n",
      "         [0.0000e+00, 1.1300e+03, 4.9451e+02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 29.2052,  -0.2430,  -1.5734, -10.5003,  13.6868,  -1.7973,  91.8966,\n",
      "         129.3455,   6.8577]], device='cuda:0')}, 'boxes_rend': tensor([[ -82.7833, -328.0853,   29.7311, -109.3610]], device='cuda:0'), 'boxes_crop': tensor([[-276.2977, -372.5555,  138.7577,  -61.2640]], device='cuda:0')}}\n",
      "1 day, 22:58:02.074481 - batched_model_predictions tensor([1]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:02.111302 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:02.240009 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:02.260036 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.4660,  0.5019,  0.7287, -2.2609],\n",
      "         [-0.7560, -0.6538, -0.0331, -1.9796],\n",
      "         [ 0.4598, -0.5663,  0.6840,  1.7072],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[ -0.4412,   0.4768,   0.7602, -23.1370],\n",
      "         [ -0.6614,  -0.7454,   0.0837, -19.7125],\n",
      "         [  0.6065,  -0.4659,   0.6442,  18.2425],\n",
      "         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'K_crop': tensor([[[2.6483e+03, 0.0000e+00, 3.6668e+03],\n",
      "         [0.0000e+00, 2.6483e+03, 3.1905e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 46.9202,  -0.2324,  -2.2083, -17.5485,  20.7374,  -2.8545, 148.4230,\n",
      "         209.2485,  10.6858]], device='cuda:0')}, 'boxes_rend': tensor([[-1582.4642, -1438.2427, -1518.2758, -1394.2928]], device='cuda:0'), 'boxes_crop': tensor([[-1670.0714, -1496.4623, -1492.9764, -1363.6412]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[ -0.4412,   0.4768,   0.7602, -23.1370],\n",
      "         [ -0.6614,  -0.7454,   0.0837, -19.7125],\n",
      "         [  0.6065,  -0.4659,   0.6442,  18.2425],\n",
      "         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[ -0.4491,   0.4834,   0.7514, -22.8794],\n",
      "         [ -0.6634,  -0.7438,   0.0820, -19.4912],\n",
      "         [  0.5985,  -0.4616,   0.6547,  18.0437],\n",
      "         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'K_crop': tensor([[[2.8626e+04, 0.0000e+00, 3.6466e+04],\n",
      "         [0.0000e+00, 2.8626e+04, 3.1053e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 3.4449e+00,  1.4485e-03,  4.6719e-02, -2.7638e-01,  2.4654e+00,\n",
      "          3.6942e-03,  8.7000e+00,  1.0370e+01,  9.8910e-01]], device='cuda:0')}, 'boxes_rend': tensor([[-1499.5476, -1314.9982, -1493.5320, -1310.8085]], device='cuda:0'), 'boxes_crop': tensor([[-1507.5752, -1320.3933, -1491.1914, -1308.1055]], device='cuda:0')}}\n",
      "1 day, 22:58:02.262324 - batched_model_predictions tensor([2]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:02.299959 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:02.426294 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:02.446106 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.1799,  0.7491,  0.6376,  0.0030],\n",
      "         [-0.9803, -0.1904, -0.0529, -0.0016],\n",
      "         [ 0.0818, -0.6345,  0.7686, -0.0745],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.1902,  0.7518,  0.6314,  0.0016],\n",
      "         [-0.9780, -0.2012, -0.0551, -0.0030],\n",
      "         [ 0.0856, -0.6280,  0.7735, -0.0710],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[607.2506,   0.0000, 401.2878],\n",
      "         [  0.0000, 607.2506, 323.7209],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 3.9049, -0.0384,  0.0287, -0.5176,  2.5929, -0.0176, 10.3929, 12.9403,\n",
      "          0.9529]], device='cuda:0')}, 'boxes_rend': tensor([[-419.2448, -430.2801,  -28.9633,  -16.5561]], device='cuda:0'), 'boxes_crop': tensor([[-610.2445, -513.0311,  162.0991,   66.2265]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-0.1902,  0.7518,  0.6314,  0.0016],\n",
      "         [-0.9780, -0.2012, -0.0551, -0.0030],\n",
      "         [ 0.0856, -0.6280,  0.7735, -0.0710],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-1.9603e-01,  7.5704e-01,  6.2327e-01,  4.8803e-04],\n",
      "         [-9.7684e-01, -2.0634e-01, -5.6620e-02, -4.1685e-03],\n",
      "         [ 8.5742e-02, -6.1994e-01,  7.7995e-01, -6.8016e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'K_crop': tensor([[[606.3325,   0.0000, 403.9174],\n",
      "         [  0.0000, 606.3324, 328.2252],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 3.6461e+00, -1.8543e-02,  3.6795e-02, -3.8820e-01,  2.5032e+00,\n",
      "         -9.2552e-03,  9.5090e+00,  1.1616e+01,  9.5858e-01]], device='cuda:0')}, 'boxes_rend': tensor([[-426.9874, -442.2276,  -35.6643,  -27.8652]], device='cuda:0'), 'boxes_crop': tensor([[-618.0693, -525.1041,  155.4438,   55.0308]], device='cuda:0')}}\n",
      "1 day, 22:58:02.448454 - batched_model_predictions tensor([3]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:02.564914 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:02.691536 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:02.711782 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.2203,  0.5917,  0.7755, -0.3888],\n",
      "         [-0.9624, -0.2613, -0.0741,  0.5665],\n",
      "         [ 0.1588, -0.7627,  0.6270,  7.7759],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-2.1717e-01,  5.7482e-01,  7.8894e-01, -9.0228e-01],\n",
      "         [-9.3624e-01, -3.5134e-01, -1.7360e-03,  1.3639e+00],\n",
      "         [ 2.7619e-01, -7.3902e-01,  6.1447e-01,  1.8391e+01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'K_crop': tensor([[[ 3.2494e+04,  0.0000e+00,  1.7841e+03],\n",
      "         [ 0.0000e+00,  3.2494e+04, -2.2480e+03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 9.3192e+00, -2.7588e-02, -2.0146e-01, -3.5380e+00,  4.7055e+00,\n",
      "         -4.8255e-01,  3.0382e+01,  4.2356e+01,  2.3652e+00]], device='cuda:0')}, 'boxes_rend': tensor([[284.4270, 372.4182, 290.4634, 379.6940]], device='cuda:0'), 'boxes_crop': tensor([[279.0081, 370.8717, 293.4416, 381.6968]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-2.1717e-01,  5.7482e-01,  7.8894e-01, -9.0228e-01],\n",
      "         [-9.3624e-01, -3.5134e-01, -1.7360e-03,  1.3639e+00],\n",
      "         [ 2.7619e-01, -7.3902e-01,  6.1447e-01,  1.8391e+01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'TCO_output': tensor([[[-2.2034e-01,  5.8488e-01,  7.8062e-01, -8.8978e-01],\n",
      "         [-9.3719e-01, -3.4881e-01, -3.1940e-03,  1.3508e+00],\n",
      "         [ 2.7042e-01, -7.3229e-01,  6.2500e-01,  1.8180e+01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'K_crop': tensor([[[ 7.3670e+04,  0.0000e+00,  3.7738e+03],\n",
      "         [ 0.0000e+00,  7.3670e+04, -5.3440e+03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 3.4421e+00,  1.7688e-03,  4.6219e-02, -2.7789e-01,  2.4610e+00,\n",
      "          3.7330e-03,  8.6961e+00,  1.0369e+01,  9.8852e-01]], device='cuda:0')}, 'boxes_rend': tensor([[286.8589, 376.4895, 289.3593, 379.6966]], device='cuda:0'), 'boxes_crop': tensor([[284.4120, 375.8074, 290.7784, 380.5821]], device='cuda:0')}}\n",
      "1 day, 22:58:02.714116 - batched_model_predictions tensor([4]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:02.828378 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:02.943520 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:02.962875 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.1651,  0.6757,  0.7185, -0.0635],\n",
      "         [-0.9824, -0.1770, -0.0593, -0.0513],\n",
      "         [ 0.0871, -0.7156,  0.6930,  0.1478],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-1.5216e-01,  6.5170e-01,  7.4306e-01, -6.9890e-04],\n",
      "         [-9.5425e-01, -2.9265e-01,  6.1266e-02,  2.1020e-01],\n",
      "         [ 2.5739e-01, -6.9975e-01,  6.6641e-01,  8.2245e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'K_crop': tensor([[[174.5110,   0.0000, 234.4991],\n",
      "         [  0.0000, 174.5110, 180.1271],\n",
      "         [  0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[ 2.3469e+01,  8.9469e-02, -8.5691e-01, -8.8445e+00,  1.0755e+01,\n",
      "         -1.5254e+00,  7.4851e+01,  1.0523e+02,  5.5648e+00]], device='cuda:0')}, 'boxes_rend': tensor([[-1225.6182,  -959.5593,  -785.1144,   129.0028]], device='cuda:0'), 'boxes_crop': tensor([[-1614.1577, -1247.5105,  1073.3862,   768.1475]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-1.5216e-01,  6.5170e-01,  7.4306e-01, -6.9890e-04],\n",
      "         [-9.5425e-01, -2.9265e-01,  6.1266e-02,  2.1020e-01],\n",
      "         [ 2.5739e-01, -6.9975e-01,  6.6641e-01,  8.2245e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0'), 'TCO_output': tensor([[[-0.1340,  0.6345,  0.7612,  0.0807],\n",
      "         [-0.9136, -0.3767,  0.1532,  0.8343],\n",
      "         [ 0.3839, -0.6749,  0.6302,  2.8092],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[ 1.3127e+03,  0.0000e+00,  1.6062e+02],\n",
      "         [ 0.0000e+00,  1.3127e+03, -2.1599e+02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[12.1068,  0.0895, -0.3633, -4.4631,  5.6736, -0.6177, 38.8424, 54.3751,\n",
      "          3.4156]], device='cuda:0')}, 'boxes_rend': tensor([[230.6502, 583.7214, 329.5833, 730.3400]], device='cuda:0'), 'boxes_crop': tensor([[179.6084, 510.1043, 536.9005, 778.0734]], device='cuda:0')}}\n",
      "1 day, 22:58:02.965205 - batched_model_predictions tensor([5]) [0] torch.Size([1, 3, 540, 720])\n",
      "1 day, 22:58:03.002467 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:03.116437 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:03.135669 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.3852,  0.5989,  0.7021, -0.7699],\n",
      "         [-0.8390, -0.5441,  0.0039, -0.7417],\n",
      "         [ 0.3843, -0.5875,  0.7121,  0.7183],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.3736,  0.5963,  0.7105, -2.2647],\n",
      "         [-0.7899, -0.6061,  0.0933, -2.1507],\n",
      "         [ 0.4863, -0.5263,  0.6975,  2.1801],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[1.0888e+03, 0.0000e+00, 1.3267e+03],\n",
      "         [0.0000e+00, 1.0888e+03, 1.2439e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[11.1402,  0.0751, -0.1435, -4.2825,  5.2985, -0.5835, 36.0920, 50.2941,\n",
      "          3.0353]], device='cuda:0')}, 'boxes_rend': tensor([[-1359.5114, -1359.4385, -1210.9572, -1238.9420]], device='cuda:0'), 'boxes_crop': tensor([[-1426.9883, -1405.5887,  -996.2527, -1082.5370]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-0.3736,  0.5963,  0.7105, -2.2647],\n",
      "         [-0.7899, -0.6061,  0.0933, -2.1507],\n",
      "         [ 0.4863, -0.5263,  0.6975,  2.1801],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.3460,  0.5886,  0.7307, -8.6374],\n",
      "         [-0.7146, -0.6699,  0.2013, -8.1458],\n",
      "         [ 0.6079, -0.4525,  0.6524,  8.4320],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[3.4507e+03, 0.0000e+00, 3.7441e+03],\n",
      "         [0.0000e+00, 3.4507e+03, 3.5236e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[15.2165,  0.1603, -0.5097, -6.0747,  6.9555, -0.8289, 49.8670, 70.5319,\n",
      "          3.8677]], device='cuda:0')}, 'boxes_rend': tensor([[-1209.6073, -1212.7710, -1161.2039, -1173.1708]], device='cuda:0'), 'boxes_crop': tensor([[-1230.9973, -1227.3335, -1095.0801, -1125.3958]], device='cuda:0')}}\n",
      "1 day, 22:58:03.137950 - batched_model_predictions tensor([6]) [0] torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "pose model torch.Size([1, 3, 540, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 day, 22:58:03.175230 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=1.pth.tar\n",
      "1 day, 22:58:03.289540 - Wrote debug data: /home/ubuntu/synthetic_pose_estimation/cosypose/local_data/debug_data/debug_iter=2.pth.tar\n",
      "1 day, 22:58:03.308748 - outputs, {'iteration=1': {'TCO_input': tensor([[[-0.3751,  0.5573,  0.7408, -0.8288],\n",
      "         [-0.8460, -0.5325, -0.0277, -0.6108],\n",
      "         [ 0.3790, -0.6371,  0.6712,  0.8562],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[-0.3556,  0.5493,  0.7562, -2.8986],\n",
      "         [-0.7925, -0.6061,  0.0676, -2.0704],\n",
      "         [ 0.4955, -0.5752,  0.6509,  3.0963],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'K_crop': tensor([[[1.4266e+03, 0.0000e+00, 1.5404e+03],\n",
      "         [0.0000e+00, 1.4266e+03, 1.1372e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[13.9679,  0.1339, -0.3385, -5.2976,  6.4993, -0.7384, 45.3918, 63.7970,\n",
      "          3.6162]], device='cuda:0')}, 'boxes_rend': tensor([[-1176.6245,  -849.1419, -1058.6484,  -772.9935]], device='cuda:0'), 'boxes_crop': tensor([[-1223.5916,  -899.3744,  -894.8223,  -652.7975]], device='cuda:0')}, 'iteration=2': {'TCO_input': tensor([[[-0.3556,  0.5493,  0.7562, -2.8986],\n",
      "         [-0.7925, -0.6061,  0.0676, -2.0704],\n",
      "         [ 0.4955, -0.5752,  0.6509,  3.0963],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0'), 'TCO_output': tensor([[[ -0.3319,   0.5378,   0.7750, -12.0943],\n",
      "         [ -0.7132,  -0.6808,   0.1669,  -8.5493],\n",
      "         [  0.6174,  -0.4974,   0.6095,  13.0572],\n",
      "         [  0.0000,   0.0000,   0.0000,   1.0000]]], device='cuda:0'), 'K_crop': tensor([[[5.4639e+03, 0.0000e+00, 5.2745e+03],\n",
      "         [0.0000e+00, 5.4638e+03, 3.7730e+03],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0'), 'model_outputs': {'pose': tensor([[16.7765,  0.0988, -0.5350, -6.4852,  7.7803, -0.9544, 54.0874, 76.0432,\n",
      "          4.2170]], device='cuda:0')}, 'boxes_rend': tensor([[-1043.2275,  -730.4111, -1011.7446,  -709.8492]], device='cuda:0'), 'boxes_crop': tensor([[-1055.4901,  -742.7300,  -969.6521,  -678.3515]], device='cuda:0')}}\n",
      "100%|| 1/1 [00:06<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=Tesla T4/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 510.73.08\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 510.73.08\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = Tesla T4/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:06<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n",
      "ven = NVIDIA Corporation\n",
      "torch.Size([1, 3, 540, 720]) torch.Size([1, 3, 240, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 day, 22:58:02.165951 - preds,pix2pose_detections, {'detections': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'coarse/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=2': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    initial_bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      ")}\n",
      "1 day, 20:59:41.305106 - preds,pix2pose_detections, {'detections': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'coarse/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=2': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    initial_bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      ")}\n",
      "0:00:15.162980 - preds,pix2pose_detections, {'detections': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'coarse/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=1': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      "), 'refiner/iteration=2': PandasTensorCollection(\n",
      "    poses: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    poses_input: torch.Size([7, 4, 4]) torch.float32 cpu,\n",
      "    K_crop: torch.Size([7, 3, 3]) torch.float32 cpu,\n",
      "    boxes_rend: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    boxes_crop: torch.Size([7, 4]) torch.float32 cpu,\n",
      "    initial_bboxes: torch.Size([7, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "   scene_id  view_id  score       label  det_id  batch_im_id  group_id\n",
      "0         0       20      1  obj_000000     140            0         0\n",
      "1         0       20      1  obj_000004     141            0         0\n",
      "2         0       20      1  obj_000005     142            0         0\n",
      "3         0       20      1  obj_000006     143            0         0\n",
      "4         0       20      1  obj_000007     144            0         0\n",
      "5         0       20      1  obj_000008     145            0         0\n",
      "6         0       20      1  obj_000009     146            0         0\n",
      ")}\n",
      "1 day, 22:58:02.167735 - Done with predictions\n",
      "1 day, 20:59:41.306890 - Done with predictions\n",
      "0:00:15.164764 - Done with predictions\n",
      "/home/ubuntu/anaconda3/envs/cosypose/lib/python3.7/site-packages/trimesh/exchange/ply.py:501: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in lines])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detections PandasTensorCollection(\n",
      "    poses: torch.Size([14917, 4, 4]) torch.float32 cpu,\n",
      "    bboxes: torch.Size([14917, 4]) torch.float32 cpu,\n",
      "----------------------------------------\n",
      "    infos:\n",
      "       scene_id  view_id  score       label\n",
      "0             0        0      1  obj_000000\n",
      "1             0        0      1  obj_000004\n",
      "2             0        0      1  obj_000005\n",
      "3             0        0      1  obj_000006\n",
      "4             0        0      1  obj_000007\n",
      "...         ...      ...    ...         ...\n",
      "14912         2      998      1  obj_000005\n",
      "14913         2      998      1  obj_000006\n",
      "14914         2      998      1  obj_000007\n",
      "14915         2      998      1  obj_000008\n",
      "14916         2      998      1  obj_000009\n",
      "\n",
      "[14917 rows x 4 columns]\n",
      ")\n",
      "ds_name bracket_assembly\n",
      "object_ds_name bracket_assembly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictor, mesh_db = load_models(coarse_run_id, refiner_run_id, n_workers=n_plotters, object_set=object_set)\n",
    "\n",
    "mv_predictor = MultiviewScenePredictor(mesh_db)\n",
    "base_pred_kwargs = dict(\n",
    "    n_coarse_iterations=n_coarse_iterations,\n",
    "    n_refiner_iterations=n_refiner_iterations,\n",
    "    skip_mv=skip_mv,\n",
    "    pose_predictor=predictor,\n",
    "    mv_predictor=mv_predictor,\n",
    ")\n",
    "\n",
    "if 'bracket_assembly' in ds_name:\n",
    "    bracket_detections = load_custom_detection_from_gt().cpu()\n",
    "    pred_kwargs = {\n",
    "        'pix2pose_detections': dict(\n",
    "            detections=bracket_detections,\n",
    "            **base_pred_kwargs\n",
    "        )\n",
    "    }\n",
    "\n",
    "scene_ds_pred = MultiViewWrapper(scene_ds, n_views=n_views)\n",
    "if group_id is not None:\n",
    "    mask = scene_ds_pred.frame_index['group_id'] == group_id\n",
    "    scene_ds_pred.frame_index = scene_ds_pred.frame_index[mask].reset_index(drop=True)\n",
    "elif n_groups is not None:\n",
    "    scene_ds_pred.frame_index = scene_ds_pred.frame_index[:n_groups]\n",
    "\n",
    "pred_runner = MultiviewPredictionRunner(\n",
    "    scene_ds_pred, batch_size=1, n_workers=n_workers,\n",
    "    cache_data=len(pred_kwargs) > 1)\n",
    "\n",
    "all_predictions = dict()\n",
    "for pred_prefix, pred_kwargs_n in pred_kwargs.items():\n",
    "    preds = pred_runner.get_predictions(**pred_kwargs_n)\n",
    "    # logger.info(f\"preds,{pred_prefix}, {preds}\")\n",
    "    for preds_name, preds_n in preds.items():\n",
    "        all_predictions[f'{pred_prefix}/{preds_name}'] = preds_n\n",
    "\n",
    "logger.info(\"Done with predictions\")\n",
    "# torch.distributed.barrier()\n",
    "\n",
    "# Evaluation\n",
    "predictions_to_evaluate = set()\n",
    "if 'ycbv' in ds_name:\n",
    "    det_key = 'posecnn_init'\n",
    "    all_predictions['posecnn'] = posecnn_detections\n",
    "    predictions_to_evaluate.add('posecnn')\n",
    "    predictions_to_evaluate.add(f'{det_key}/refiner/iteration={n_refiner_iterations}')\n",
    "elif 'tless' in ds_name:\n",
    "    det_key = 'pix2pose_detections'\n",
    "    predictions_to_evaluate.add(f'{det_key}/refiner/iteration={n_refiner_iterations}')\n",
    "elif 'bracket_assembly' in ds_name: # BOP dataset\n",
    "    det_key = 'pix2pose_detections'\n",
    "    predictions_to_evaluate.add(f'{det_key}/coarse/iteration=1')\n",
    "    predictions_to_evaluate.add(f'{det_key}/refiner/iteration={n_refiner_iterations}')\n",
    "else:\n",
    "    raise ValueError(ds_name)\n",
    "\n",
    "if n_views > 1:\n",
    "    for k in [\n",
    "            # f'ba_input',\n",
    "            # f'ba_output',\n",
    "            f'ba_output+all_cand'\n",
    "    ]:\n",
    "        predictions_to_evaluate.add(f'{det_key}/{k}')\n",
    "\n",
    "all_predictions = OrderedDict({k: v for k, v in sorted(all_predictions.items(), key=lambda item: item[0])})\n",
    "# Evaluation.\n",
    "meters = get_pose_meters(scene_ds)\n",
    "mv_group_ids = list(iter(pred_runner.sampler))\n",
    "scene_ds_ids = np.concatenate(scene_ds_pred.frame_index.loc[mv_group_ids, 'scene_ds_ids'].values)\n",
    "sampler = ListSampler(scene_ds_ids)\n",
    "eval_runner = PoseEvaluation(scene_ds, meters, n_workers=n_workers,\n",
    "                                cache_data=True, batch_size=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a428e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_617169/1285950121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobject_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_object_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmesh_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeshDataBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_object_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmeshes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeshes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTCO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "scene_id = 0\n",
    "frame_id = 20\n",
    "object_ds_name = 'bracket_assembly'\n",
    "object_ds = make_object_dataset(object_ds_name)\n",
    "mesh_db = MeshDataBase.from_object_ds(object_ds).batched().cuda().float()\n",
    "meshes = mesh_db.select(labels)\n",
    "points = meshes.sample_points(2000, deterministic=True)\n",
    "uv = project_points(points, K, TCO)\n",
    "boxes_rend = boxes_from_uv(uv)\n",
    "boxes_crop, images_cropped = deepim_crops(\n",
    "    images=images, obs_boxes=boxes_rend, K=K,\n",
    "    TCO_pred=TCO, O_vertices=points, output_size=self.render_size, lamb=1.4\n",
    ")\n",
    "plot(image)\n",
    "plot(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c584381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
